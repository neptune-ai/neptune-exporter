#
# Copyright (c) 2025, Neptune Labs Sp. z o.o.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Minfx loader for importing parquet-exported data using Neptune-SDK compatible API.

This loader uses minfx to import data that was previously exported from Neptune,
allowing migration between Neptune-SDK compatible backends.

Implementation Notes (from debugging session):
----------------------------------------------

1. **Preventing Neptune from adding extra attributes**:
   Neptune's init_run() automatically creates monitoring/{hash}/hostname, pid, tid
   attributes when ANY of these capture options is True:
   - capture_stdout
   - capture_stderr
   - capture_traceback (defaults to True!)
   - capture_hardware_metrics

   We must set ALL of them to False to prevent extra monitoring/* attributes.
   See Neptune source: metadata_containers/run.py lines 497-498:
   ```
   if any((capture_stderr, capture_stdout, capture_traceback, capture_hardware_metrics)):
       self._write_initial_monitoring_attributes()
   ```

2. **Git tracking creates unwanted diffs**:
   Setting git_ref=False prevents Neptune from creating source_code/diff and
   source_code/upstream_diff* files which would be extra data not in the original.

3. **sys/custom_run_id vs import/original_run_id**:
   We don't use Neptune's custom_run_id parameter because it automatically creates
   a sys/custom_run_id attribute. Instead, we store the original run ID in
   import/original_run_id namespace to enable import run tracking without polluting sys/.

4. **File extension detection for UI display**:
   Neptune's UI requires file extensions to properly render images (PNG, JPEG, etc.).
   Files without extensions appear as generic downloads. We detect file types via
   magic bytes and use File.from_path(..., extension=ext) to ensure proper UI display.

5. **Artifact registration via internal APIs**:
   Neptune artifacts are references (hash + metadata), not actual file content.
   The exporter only saves files_list.json with metadata. To recreate artifacts
   in Neptune UI, we use internal APIs (create_new_artifact, AssignArtifact) to
   register the artifact by its hash without needing the original files.

6. **Skipping auto-generated source_code paths**:
   source_code/diff and source_code/upstream_diff* are auto-generated by Neptune
   when source files are uploaded. We skip these during import as Neptune will
   regenerate them if needed (though we disable git tracking, so they won't appear).

7. **sys/hostname exception**:
   While we skip most sys/* attributes (managed by Neptune), we explicitly allow
   sys/hostname to be copied from the original data for provenance tracking.
"""

import json
import logging
import os
import shutil
import tempfile
import zipfile
from pathlib import Path
from typing import Any, Generator, Optional

import pandas as pd
import pyarrow as pa

# Allow OAuth over HTTP for local development/testing.
# Must be set BEFORE importing Neptune to avoid InsecureTransportError.
os.environ.setdefault("OAUTHLIB_INSECURE_TRANSPORT", "1")

import minfx.neptune_v2 as neptune
from neptune.attributes.series.float_series import FloatSeries as NeptuneFloatSeriesAttr
from neptune.attributes.series.string_series import (
    StringSeries as NeptuneStringSeriesAttr,
)
from neptune.types import File, FloatSeries, StringSeries, StringSet

from neptune_exporter.loaders.loader import DataLoader
from neptune_exporter.types import ProjectId, TargetExperimentId, TargetRunId

# PERFORMANCE: Increase batch size for series operations.
# Default is 1 for FloatSeries, 10 for StringSeries - this creates N individual
# operations internally. By increasing to 1024, we reduce internal operations.
NeptuneFloatSeriesAttr.max_batch_size = 1024  # type: ignore[attr-defined]
NeptuneStringSeriesAttr.max_batch_size = 1024  # type: ignore[attr-defined]


# Parameter types that are set directly as run[path] = value
_PARAMETER_TYPES = {"float", "int", "string", "bool", "datetime", "string_set"}

# Series types that use run[path].append()
_FLOAT_SERIES_TYPE = "float_series"
_STRING_SERIES_TYPE = "string_series"

# File types
_FILE_TYPE = "file"
_FILE_SERIES_TYPE = "file_series"
_FILE_SET_TYPE = "file_set"
_ARTIFACT_TYPE = "artifact"

# Skipped types (no native support in minfx)
_HISTOGRAM_SERIES_TYPE = "histogram_series"

# File type detection magic bytes
# Neptune UI requires file extensions to properly display images.
# Files uploaded without extensions appear as generic downloads.
# We detect file types from magic bytes to ensure proper UI rendering.
_FILE_SIGNATURES = {
    b"\x89PNG\r\n\x1a\n": ".png",
    b"\xff\xd8\xff": ".jpg",
    b"GIF87a": ".gif",
    b"GIF89a": ".gif",
    b"RIFF": ".webp",  # WebP starts with RIFF, need to check further
    b"PK\x03\x04": ".zip",
    b"{": ".json",  # JSON typically starts with {
    b"[": ".json",  # JSON array starts with [
}


def _detect_file_extension(file_path: Path) -> Optional[str]:
    """Detect file extension from magic bytes.

    Args:
        file_path: Path to the file to detect

    Returns:
        File extension (e.g., ".png") or None if unknown
    """
    try:
        with open(file_path, "rb") as f:
            header = f.read(16)

        for signature, extension in _FILE_SIGNATURES.items():
            if header.startswith(signature):
                # Special case for WebP: check for WEBP marker
                if signature == b"RIFF" and b"WEBP" not in header:
                    continue
                return extension

        return None
    except Exception:
        return None


def _upload_source_code(run, zip_path: Path) -> None:
    """Upload source code files from a zip to Neptune's source_code/files namespace."""
    with tempfile.TemporaryDirectory() as temp_dir:
        with zipfile.ZipFile(zip_path, "r") as zf:
            zf.extractall(temp_dir)
        run["source_code/files"].upload_files(temp_dir, wait=True)


def _register_artifact_from_metadata(
    run, attribute_path: str, files_list: list, logger: logging.Logger
) -> bool:
    """Register an artifact in Neptune using metadata from exported files_list.json.

    BACKGROUND: Neptune artifacts are references (hash + metadata), not actual files.
    The exporter saves only files_list.json containing metadata about the artifact's
    files. To recreate the artifact in Neptune UI (so it appears as a proper artifact
    widget, not just JSON text), we must use Neptune's internal APIs.

    This function:
    1. Parses the files_list.json metadata
    2. Computes the artifact hash using Neptune's FileHasher
    3. Calls create_new_artifact to register the artifact in Neptune's artifact service
    4. Uploads file metadata if the artifact is new
    5. Uses AssignArtifact operation to link the artifact to the attribute path

    If any step fails, the caller should fall back to _log_artifact_metadata_as_strings
    which stores the metadata as regular string attributes (less pretty but functional).

    Args:
        run: Neptune run object
        attribute_path: The attribute path (e.g., "artifacts/best_model")
        files_list: List of file metadata dicts from files_list.json
        logger: Logger instance for debug output

    Returns:
        True if artifact was registered successfully, False otherwise
    """
    try:
        from neptune.internal.artifacts.file_hasher import FileHasher
        from neptune.internal.artifacts.types import (
            ArtifactFileData,
            ArtifactMetadataSerializer,
        )
        from neptune.internal.backends.hosted_artifact_operations import (
            create_new_artifact,
            upload_artifact_files_metadata,
        )
        from neptune.internal.backends.hosted_client import DEFAULT_REQUEST_KWARGS
        from neptune.internal.operation import AssignArtifact
    except ImportError as e:
        logger.warning(
            f"Failed to import Neptune internal modules for artifact {attribute_path}: {e}. "
            "Falling back to string metadata."
        )
        return False

    try:
        # Convert files_list to ArtifactFileData objects
        artifact_files = []
        for file_data in files_list:
            metadata = ArtifactMetadataSerializer.deserialize(
                file_data.get("metadata", [])
            )
            artifact_file = ArtifactFileData(
                file_path=file_data["filePath"],
                file_hash=file_data["fileHash"],
                type=file_data["type"],
                size=file_data.get("size"),
                metadata=metadata,
            )
            artifact_files.append(artifact_file)

        # Compute artifact hash
        artifact_hash = FileHasher.get_artifact_hash(artifact_files)
        artifact_size = sum(f.size or 0 for f in artifact_files)

        logger.info(
            f"Registering artifact {attribute_path} with hash {artifact_hash[:16]}..."
        )

        # Get backend and project info
        backend = run._backend
        artifacts_client = backend.artifacts_client
        project_id = run._project_id
        parent_identifier = run._id

        # Register the artifact with Neptune's artifact service
        artifact = create_new_artifact(
            swagger_client=artifacts_client,
            project_id=project_id,
            artifact_hash=artifact_hash,
            parent_identifier=parent_identifier,
            size=artifact_size,
            default_request_params=DEFAULT_REQUEST_KWARGS,
        )

        # Upload file metadata if not already present
        if not artifact.received_metadata:
            upload_artifact_files_metadata(
                swagger_client=artifacts_client,
                project_id=project_id,
                artifact_hash=artifact_hash,
                files=artifact_files,
                default_request_params=DEFAULT_REQUEST_KWARGS,
            )

        # Assign the artifact to the attribute path
        # NOTE: wait=False allows artifact assignment to be batched with other operations
        # The final sync() before run.stop() ensures all operations complete
        path_list = attribute_path.split("/")
        with run._lock:
            run._op_processor.enqueue_operation(
                AssignArtifact(path=path_list, hash=artifact_hash), wait=False
            )

        logger.info(f"Successfully registered artifact {attribute_path}")
        return True

    except (KeyError, TypeError, AttributeError) as e:
        logger.warning(
            f"Failed to register artifact {attribute_path} due to data error: {e}. "
            "Falling back to string metadata."
        )
        return False
    except Exception as e:
        logger.warning(
            f"Failed to register artifact {attribute_path} via API: {e}. "
            "Falling back to string metadata."
        )
        return False


def _log_artifact_metadata_as_strings(
    run, attribute_path: str, files_list: list
) -> None:
    """Fallback: Log artifact metadata as structured string attributes."""
    for i, file_data in enumerate(files_list):
        base_path = f"{attribute_path}/{i}" if len(files_list) > 1 else attribute_path

        if "filePath" in file_data:
            run[f"{base_path}/filePath"] = file_data["filePath"]
        if "fileHash" in file_data:
            run[f"{base_path}/fileHash"] = file_data["fileHash"]
        if "type" in file_data:
            run[f"{base_path}/type"] = file_data["type"]
        if "size" in file_data:
            run[f"{base_path}/size"] = file_data["size"]

        if "metadata" in file_data and file_data["metadata"]:
            for meta_item in file_data["metadata"]:
                if (
                    isinstance(meta_item, dict)
                    and "key" in meta_item
                    and "value" in meta_item
                ):
                    key = meta_item["key"].replace("/", "_")
                    run[f"{base_path}/metadata/{key}"] = str(meta_item["value"])


class MinfxLoader(DataLoader):
    """Minfx loader for importing parquet-exported data using Neptune-SDK compatible API."""

    def __init__(
        self,
        project: str,
        api_token: Optional[str] = None,
        name_prefix: Optional[str] = None,
        show_client_logs: bool = False,
    ):
        """
        Initialize Minfx loader.

        Args:
            project: Target Minfx project path (workspace/project)
            api_token: Optional Minfx API token for authentication
            name_prefix: Optional prefix for run names
            show_client_logs: Enable verbose client logging
        """
        self._project = project
        self._api_token = api_token
        self._name_prefix = name_prefix
        self._logger = logging.getLogger(__name__)
        self._active_run: Optional[Any] = None  # neptune.Run type

        # Configure Neptune logging
        if show_client_logs:
            logging.getLogger("neptune").setLevel(logging.INFO)
        else:
            logging.getLogger("neptune").setLevel(logging.ERROR)

    def _get_run_name(self, run_name: str) -> str:
        """Get Neptune run name with optional prefix."""
        if self._name_prefix:
            return f"{self._name_prefix}_{run_name}"
        return run_name

    def _convert_step(self, step: Any) -> float:
        """Unmangle: convert Decimal step back to float for Neptune API."""
        if pd.notna(step):
            return float(step)
        return 0.0

    # Type map for value extraction: attr_type -> (column_name, converter_func)
    _TYPE_MAP: dict[str, tuple[str, type | Any]] = {
        "int": ("int_value", int),
        "float": ("float_value", float),
        "string": ("string_value", lambda x: x),
        "bool": ("bool_value", bool),
        "datetime": ("datetime_value", lambda x: x),
        "string_set": (
            "string_set_value",
            lambda x: StringSet(set(x)) if x is not None else None,
        ),
        _FLOAT_SERIES_TYPE: ("float_value", float),
        _STRING_SERIES_TYPE: ("string_value", lambda x: x),
    }

    def _get_value_for_tuple(self, row: tuple, attr_type: str) -> Any:
        """Extract value from namedtuple row (from itertuples). Faster than Series access."""
        if attr_type not in self._TYPE_MAP:
            return None

        column, converter = self._TYPE_MAP[attr_type]
        value = getattr(row, column, None)

        # string_set uses None check instead of pd.notna
        if attr_type == "string_set":
            return converter(value) if value is not None else None

        return converter(value) if pd.notna(value) else None

    def create_experiment(
        self, project_id: ProjectId, experiment_name: str
    ) -> TargetExperimentId:
        """
        Neptune doesn't have a separate experiment concept - runs belong to projects.
        We return the experiment_name as a tag/group identifier.
        """
        return TargetExperimentId(experiment_name)

    def find_run(
        self,
        project_id: ProjectId,
        run_name: str,
        experiment_id: Optional[TargetExperimentId],
    ) -> Optional[TargetRunId]:
        """Find a run by import/original_run_id in the target Neptune project.

        Args:
            project_id: Original Neptune project ID (unused, we use self._project)
            run_name: Name of the run to find
            experiment_id: Experiment name (unused for Neptune)

        Returns:
            Neptune run sys/id if found, None otherwise
        """
        prefixed_name = self._get_run_name(run_name)

        try:
            # Use init_project to query runs table
            with neptune.init_project(
                api_token=self._api_token,
                project=self._project,
                mode="read-only",
            ) as project:
                runs_table = project.fetch_runs_table(
                    columns=["sys/id", "import/original_run_id"],
                    progress_bar=False,
                    ascending=True,
                ).to_pandas()

                if runs_table.empty:
                    return None

                # Find run with matching import/original_run_id
                if "import/original_run_id" in runs_table.columns:
                    matching = runs_table[
                        runs_table["import/original_run_id"] == prefixed_name
                    ]
                    if not matching.empty:
                        return TargetRunId(matching.iloc[0]["sys/id"])

            return None
        except ValueError as e:
            # Expected error when project is empty (no sys/creation_time column to sort by)
            # The Neptune SDK raises this when there are no runs to determine column types
            if "sys/creation_time" in str(e):
                self._logger.debug(
                    f"Project {self._project} appears empty (no runs yet)"
                )
            else:
                self._logger.warning(f"Error finding run '{prefixed_name}': {e}")
            return None
        except Exception:
            self._logger.warning(
                f"Error finding run '{prefixed_name}' in project {self._project}",
                exc_info=True,
            )
            return None

    def create_run(
        self,
        project_id: ProjectId,
        run_name: str,
        experiment_id: Optional[TargetExperimentId] = None,
        parent_run_id: Optional[TargetRunId] = None,
        fork_step: Optional[float] = None,
        step_multiplier: Optional[int] = None,
    ) -> TargetRunId:
        """Create a Neptune run.

        Args:
            project_id: Original Neptune project ID (unused, we use self._project)
            run_name: Name for the run (will be set as custom_run_id)
            experiment_id: Experiment name (will be set as sys/name)
            parent_run_id: Parent run sys/id for forking (not supported in minfx API)
            fork_step: Fork step (not supported in minfx API)
            step_multiplier: Step multiplier (unused for Neptune - supports floats)

        Returns:
            Neptune run sys/id
        """
        prefixed_name = self._get_run_name(run_name)

        try:
            # Minfx doesn't support forking via init_run
            # We'll store fork info as metadata instead
            #
            # CRITICAL: Disable ALL automatic tracking to avoid adding extra data.
            # Neptune creates monitoring/{hash}/hostname, pid, tid when ANY capture
            # option is True. The capture_traceback defaults to True, so we must
            # explicitly disable it. See module docstring for details.
            #
            # - source_files=[] : No automatic source code tracking
            # - git_ref=False : No git diffs (source_code/diff, upstream_diff)
            # - capture_*=False : No monitoring/* attributes at all
            # - custom_run_id NOT set : Avoids creating sys/custom_run_id
            self._active_run = neptune.init_run(
                project=self._project,
                api_token=self._api_token,
                name=experiment_id,  # sys/name
                source_files=[],
                git_ref=False,
                capture_hardware_metrics=False,
                capture_stdout=False,
                capture_stderr=False,
                capture_traceback=False,  # IMPORTANT: defaults to True!
            )

            # Store original run ID for finding/tracking without using sys/custom_run_id
            self._active_run["import/original_run_id"] = prefixed_name

            # Store original project ID for traceability
            self._active_run["import/original_project_id"] = str(project_id)

            # Store fork relationship as metadata if provided
            # Use import/ namespace to avoid reserved sys/ namespace
            if parent_run_id:
                self._active_run["import/forking/parent"] = str(parent_run_id)
                if fork_step is not None:
                    self._active_run["import/forking/step"] = float(fork_step)

            neptune_run_id = self._active_run["sys/id"].fetch()
            self._logger.info(
                f"Created run '{prefixed_name}' with Neptune ID {neptune_run_id}"
            )
            return TargetRunId(neptune_run_id)

        except Exception:
            self._logger.error(
                f"Error creating run '{prefixed_name}' in project {self._project}",
                exc_info=True,
            )
            raise

    def upload_run_data(
        self,
        run_data: Generator[pa.Table, None, None],
        run_id: TargetRunId,
        files_directory: Path,
        step_multiplier: int,
    ) -> None:
        """Upload all data for a single run to Neptune.

        Args:
            run_data: Generator yielding PyArrow tables with run data
            run_id: Neptune run sys/id
            files_directory: Base directory for file artifacts
            step_multiplier: Step multiplier (unused - Neptune supports float steps)
        """
        try:
            if self._active_run is None:
                self._logger.error(
                    f"Run {run_id} is not active. Call create_run first."
                )
                raise RuntimeError(f"Run {run_id} is not active")

            for run_data_part in run_data:
                run_df = run_data_part.to_pandas()

                self.upload_parameters(run_df, run_id)
                self.upload_metrics(run_df, run_id)
                self.upload_series(run_df, run_id)
                self.upload_files(run_df, run_id, files_directory)

            # Sync to ensure all uploads complete before stopping
            self._active_run.sync()

            # Stop the run (sync and close)
            self._active_run.stop()
            self._active_run = None

            self._logger.info(f"Successfully uploaded run {run_id} to Neptune")

        except Exception:
            self._logger.error(f"Error uploading data for run {run_id}", exc_info=True)
            if self._active_run:
                self._active_run.stop()
                self._active_run = None
            raise

    def upload_parameters(self, run_data: pd.DataFrame, run_id: TargetRunId) -> None:
        """Upload parameters to Neptune run.

        Minfx API: run[path] = value

        PERFORMANCE: Pre-filters sys/ attributes and uses itertuples() instead of
        iterrows() for ~10x faster iteration.
        """
        if self._active_run is None:
            raise RuntimeError("No active run")

        param_data = run_data[run_data["attribute_type"].isin(_PARAMETER_TYPES)]

        if param_data.empty:
            return

        # Pre-filter: skip sys/* except allowed ones
        # - sys/hostname: preserved for monitoring info
        # - sys/tags, sys/group_tags: user-defined tags that should be imported
        # monitoring/* attributes ARE copied to preserve original metrics.
        allowed_sys_attrs = {"sys/hostname", "sys/tags", "sys/group_tags"}
        param_data = param_data[
            (~param_data["attribute_path"].str.startswith("sys/"))
            | (param_data["attribute_path"].isin(allowed_sys_attrs))
        ]

        if param_data.empty:
            return

        # Use itertuples() for faster iteration (10-100x faster than iterrows)
        count = 0
        for row in param_data.itertuples(index=False):
            attr_path = row.attribute_path
            attr_type = row.attribute_type

            value = self._get_value_for_tuple(row, attr_type)
            if value is not None:
                try:
                    self._active_run[attr_path] = value
                    count += 1
                except Exception as e:
                    self._logger.warning(
                        f"Failed to set parameter {attr_path}: {e}",
                        exc_info=False,
                    )

        if count > 0:
            self._logger.info(f"Uploaded {count} parameters for run {run_id}")

    def _upload_series_batch(
        self,
        run_data: pd.DataFrame,
        run_id: TargetRunId,
        attr_type: str,
        value_column: str,
        series_class: type,
        label: str,
    ) -> None:
        """Upload series data (float or string) to Neptune using batch assignment.

        PERFORMANCE: Uses batch Series assignment instead of individual append() calls.
        This reduces API calls from N to M where M = number of unique series.
        """
        assert self._active_run is not None

        # Filter by type and exclude sys/ attributes
        series_data = run_data[
            (run_data["attribute_type"] == attr_type)
            & (~run_data["attribute_path"].str.startswith("sys/"))
        ]

        if series_data.empty:
            return

        series_data = series_data.sort_values(["attribute_path", "step"])

        total_points = 0
        series_count = 0

        for attr_path, group in series_data.groupby("attribute_path", sort=False):
            values = group[value_column].dropna().tolist()
            steps = group.loc[group[value_column].notna(), "step"].tolist()

            if not values:
                continue

            try:
                steps = [float(s) if pd.notna(s) else 0.0 for s in steps]
                self._active_run[attr_path] = series_class(values=values, steps=steps)
                total_points += len(values)
                series_count += 1
            except Exception:
                self._logger.warning(
                    f"Failed to upload {label} {attr_path} ({len(values)} points)",
                    exc_info=True,
                )

        if total_points > 0:
            self._logger.info(
                f"Uploaded {total_points} {label} points in {series_count} series for run {run_id}"
            )

    def upload_metrics(self, run_data: pd.DataFrame, run_id: TargetRunId) -> None:
        """Upload float series (metrics) to Neptune run."""
        if self._active_run is None:
            raise RuntimeError("No active run")
        self._upload_series_batch(
            run_data, run_id, _FLOAT_SERIES_TYPE, "float_value", FloatSeries, "metric"
        )

    def upload_series(self, run_data: pd.DataFrame, run_id: TargetRunId) -> None:
        """Upload string series to Neptune run.

        Note: histogram_series is skipped (no native minfx support)
        """
        if self._active_run is None:
            raise RuntimeError("No active run")
        self._upload_series_batch(
            run_data,
            run_id,
            _STRING_SERIES_TYPE,
            "string_value",
            StringSeries,
            "string series",
        )

        # Log warning for skipped histogram series
        histogram_data = run_data[run_data["attribute_type"] == _HISTOGRAM_SERIES_TYPE]
        if not histogram_data.empty:
            unique_paths = histogram_data["attribute_path"].nunique()
            self._logger.warning(
                f"Skipped {unique_paths} histogram_series attributes (not supported in minfx)"
            )

    def upload_files(
        self,
        run_data: pd.DataFrame,
        run_id: TargetRunId,
        files_base_path: Path,
    ) -> None:
        """Upload files and file series to Neptune run."""
        if self._active_run is None:
            raise RuntimeError("No active run")

        # Upload regular files
        file_types = [_FILE_TYPE, _FILE_SET_TYPE, _ARTIFACT_TYPE]
        file_data = run_data[run_data["attribute_type"].isin(file_types)]
        file_count = self._upload_regular_files(file_data, files_base_path)

        if file_count > 0:
            self._logger.info(f"Uploaded {file_count} files for run {run_id}")

        # Upload file series
        series_count = self._upload_file_series(run_data, files_base_path)

        if series_count > 0:
            self._logger.info(
                f"Uploaded {series_count} file series points for run {run_id}"
            )

    def _get_file_path_from_tuple(
        self, row: tuple, files_base_path: Path
    ) -> Optional[Path]:
        """Extract and validate file path from namedtuple row (from itertuples)."""
        file_value = getattr(row, "file_value", None)
        if not (pd.notna(file_value) and isinstance(file_value, dict)):
            return None

        relative_path = file_value.get("path")
        if not relative_path:
            return None

        full_path = files_base_path / relative_path
        if not full_path.exists():
            self._logger.warning(f"File not found: {full_path}")
            return None

        return full_path

    def _upload_regular_files(
        self, file_data: pd.DataFrame, files_base_path: Path
    ) -> int:
        """Upload regular files (file, file_set, artifact).

        PERFORMANCE: Pre-filters rows and uses itertuples() for faster iteration.
        """
        assert self._active_run is not None

        # Pre-filter: skip sys/ and auto-generated source_code diffs
        file_data = file_data[
            (~file_data["attribute_path"].str.startswith("sys/"))
            & (~file_data["attribute_path"].str.startswith("source_code/diff"))
            & (~file_data["attribute_path"].str.startswith("source_code/upstream_diff"))
        ]

        if file_data.empty:
            return 0

        count = 0
        for row in file_data.itertuples(index=False):
            attr_path = row.attribute_path
            attr_type = row.attribute_type

            full_path = self._get_file_path_from_tuple(row, files_base_path)
            if not full_path:
                continue

            try:
                if attr_path == "source_code/files" and attr_type == _FILE_SET_TYPE:
                    self._upload_source_code_files(full_path)
                elif attr_type == _ARTIFACT_TYPE and full_path.is_file():
                    self._upload_artifact(attr_path, full_path)
                elif full_path.is_file():
                    self._upload_single_file(attr_path, full_path)
                elif attr_type == _FILE_SET_TYPE:
                    self._upload_file_set(attr_path, full_path)
                else:
                    self._active_run[attr_path].upload_files(str(full_path), wait=True)
                count += 1
            except Exception:
                self._logger.warning(
                    f"Failed to upload file {attr_path}: {full_path}", exc_info=True
                )

        return count

    def _upload_artifact(self, attr_path: str, full_path: Path) -> None:
        """Upload artifact file (files_list.json) to Neptune."""
        assert self._active_run is not None
        try:
            with open(full_path) as f:
                files_list = json.load(f)
            if not _register_artifact_from_metadata(
                self._active_run, attr_path, files_list, self._logger
            ):
                _log_artifact_metadata_as_strings(
                    self._active_run, attr_path, files_list
                )
        except json.JSONDecodeError:
            self._active_run[attr_path].upload(str(full_path), wait=True)

    def _create_file_with_extension(self, full_path: Path) -> File:
        """Create a Neptune File object with auto-detected extension for proper UI display."""
        extension = _detect_file_extension(full_path)
        if extension:
            return File.from_path(str(full_path), extension=extension)
        return File(str(full_path))

    def _upload_single_file(self, attr_path: str, full_path: Path) -> None:
        """Upload a single file with optional extension detection.

        NOTE: wait=False allows uploads to be batched. The final sync() ensures completion.
        This is safe because full_path points to exported files (not temp files).
        """
        assert self._active_run is not None
        self._active_run[attr_path].upload(
            self._create_file_with_extension(full_path), wait=False
        )

    def _upload_file_series(self, run_data: pd.DataFrame, files_base_path: Path) -> int:
        """Upload file series data.

        PERFORMANCE: Pre-filters sys/ attributes and uses itertuples() for faster iteration.
        Note: File series cannot be batched like FloatSeries because each file is different.
        """
        assert self._active_run is not None

        # Pre-filter and sort
        file_series_data = run_data[
            (run_data["attribute_type"] == _FILE_SERIES_TYPE)
            & (~run_data["attribute_path"].str.startswith("sys/"))
        ]

        if file_series_data.empty:
            return 0

        file_series_data = file_series_data.sort_values(["attribute_path", "step"])

        count = 0
        for row in file_series_data.itertuples(index=False):
            attr_path = row.attribute_path

            full_path = self._get_file_path_from_tuple(row, files_base_path)
            if not full_path:
                continue

            try:
                step = self._convert_step(row.step)
                self._active_run[attr_path].append(
                    self._create_file_with_extension(full_path), step=step
                )
                count += 1
            except Exception:
                self._logger.warning(
                    f"Failed to append file series {attr_path} at step {row.step}",
                    exc_info=True,
                )

        return count

    def _upload_source_code_files(self, dir_path: Path) -> None:
        """Upload source code files to Neptune's source_code/files namespace."""
        assert self._active_run is not None

        files_zip = dir_path / "files.zip"
        if files_zip.exists():
            self._logger.info(f"Uploading source_code/files from {files_zip}")
            _upload_source_code(self._active_run, files_zip)
        else:
            self._logger.info(f"Uploading source_code/files from {dir_path}")
            self._active_run["source_code/files"].upload_files(str(dir_path), wait=True)

        self._logger.info("Successfully uploaded source_code/files")

    def _upload_file_set(self, attr_path: str, dir_path: Path) -> None:
        """Upload a file_set directory to Neptune.

        Neptune exports FileSets as directories containing a files.zip.
        We extract the zip to a temp directory and upload the contents.
        """
        assert self._active_run is not None

        files_zip = dir_path / "files.zip"

        if files_zip.exists() and len(list(dir_path.iterdir())) == 1:
            # Extract and upload from temp directory (wait=True ensures completion before cleanup)
            temp_dir = tempfile.mkdtemp(prefix="neptune_fileset_")
            try:
                with zipfile.ZipFile(files_zip, "r") as zf:
                    zf.extractall(temp_dir)
                self._active_run[attr_path].upload_files(temp_dir, wait=True)
            finally:
                shutil.rmtree(temp_dir, ignore_errors=True)
        else:
            # Upload directory as-is
            self._active_run[attr_path].upload_files(str(dir_path), wait=True)
